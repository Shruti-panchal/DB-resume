{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53cf4533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9998d479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "      <th>Skill</th>\n",
       "      <th>Exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>['Interactive', 'Flask', 'Security', 'Ggplot',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills  R  Python  SAP HANA  Tableau  SAP HANA...</td>\n",
       "      <td>['Telecom', 'Kpis', 'Servers', 'Sales', 'Engli...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "      <td>['Research', 'Forecasting']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n B.Tech   Rayat and Bahr...</td>\n",
       "      <td>['English', 'System', 'Python', 'Automation', ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>['Interactive', 'Flask', 'Security', 'Ggplot',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Testing</td>\n",
       "      <td>Computer Skills:   Proficient in MS office (Wo...</td>\n",
       "      <td>['Excel', 'Word', 'Engineering', 'Windows', 'T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Testing</td>\n",
       "      <td>Computer Skills:   Proficient in MS office (Wo...</td>\n",
       "      <td>['Excel', 'Word', 'Engineering', 'Windows', 'T...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Testing</td>\n",
       "      <td>Willingness to accept the challenges.  Positi...</td>\n",
       "      <td>['Hardware', 'Analyze', 'Engineering', 'Regula...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Testing</td>\n",
       "      <td>COMPUTER SKILLS &amp; SOFTWARE KNOWLEDGE MS-Power ...</td>\n",
       "      <td>['Hardware', 'Testing', 'Electronics', 'C', 'M...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Testing</td>\n",
       "      <td>Skill Set OS Windows XP/7/8/8.1/10 Database MY...</td>\n",
       "      <td>['Security', 'Windows', 'Database', 'Html', 'O...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Category                                             Resume  \\\n",
       "0    Data Science  Skills * Programming Languages: Python (pandas...   \n",
       "1    Data Science  Skills  R  Python  SAP HANA  Tableau  SAP HANA...   \n",
       "2    Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...   \n",
       "3    Data Science  Education Details \\r\\n B.Tech   Rayat and Bahr...   \n",
       "4    Data Science  Skills * Programming Languages: Python (pandas...   \n",
       "..            ...                                                ...   \n",
       "179       Testing  Computer Skills:   Proficient in MS office (Wo...   \n",
       "180       Testing  Computer Skills:   Proficient in MS office (Wo...   \n",
       "181       Testing   Willingness to accept the challenges.  Positi...   \n",
       "182       Testing  COMPUTER SKILLS & SOFTWARE KNOWLEDGE MS-Power ...   \n",
       "183       Testing  Skill Set OS Windows XP/7/8/8.1/10 Database MY...   \n",
       "\n",
       "                                                 Skill  Exp  \n",
       "0    ['Interactive', 'Flask', 'Security', 'Ggplot',...  NaN  \n",
       "1    ['Telecom', 'Kpis', 'Servers', 'Sales', 'Engli...  NaN  \n",
       "2                          ['Research', 'Forecasting']  NaN  \n",
       "3    ['English', 'System', 'Python', 'Automation', ...  NaN  \n",
       "4    ['Interactive', 'Flask', 'Security', 'Ggplot',...  NaN  \n",
       "..                                                 ...  ...  \n",
       "179  ['Excel', 'Word', 'Engineering', 'Windows', 'T...  NaN  \n",
       "180  ['Excel', 'Word', 'Engineering', 'Windows', 'T...  NaN  \n",
       "181  ['Hardware', 'Analyze', 'Engineering', 'Regula...  NaN  \n",
       "182  ['Hardware', 'Testing', 'Electronics', 'C', 'M...  NaN  \n",
       "183  ['Security', 'Windows', 'Database', 'Html', 'O...  NaN  \n",
       "\n",
       "[184 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"final.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc39df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Data Science', 'HR', 'Advocate', 'Arts', 'web Designing',\n",
       "       'Mechanical Engineer', 'Sales', 'Health and fitness',\n",
       "       'Civil Engineer', 'Java Developer', 'Business Analyst',\n",
       "       'SAP Developer', 'Automation Testing', 'Electrical Engineering',\n",
       "       'Operations Manager', 'Python Developer', 'DevOps Engineer',\n",
       "       'Network Security Engineer', 'PMO', 'Database', 'Hadoop',\n",
       "       'ETL Developer', 'DotNet Developer', 'Blockchain', 'Testing'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1811b553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Resume</th>\n",
       "      <th>Skill</th>\n",
       "      <th>Exp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>['Interactive', 'Flask', 'Security', 'Ggplot',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills  R  Python  SAP HANA  Tableau  SAP HANA...</td>\n",
       "      <td>['Telecom', 'Kpis', 'Servers', 'Sales', 'Engli...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\nMay 2013 to May 2017 B.E...</td>\n",
       "      <td>['Research', 'Forecasting']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n B.Tech   Rayat and Bahr...</td>\n",
       "      <td>['English', 'System', 'Python', 'Automation', ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills * Programming Languages: Python (pandas...</td>\n",
       "      <td>['Interactive', 'Flask', 'Security', 'Ggplot',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Areas of Interest Deep Learning, Control Syste...</td>\n",
       "      <td>['Flask', 'System', 'Electronics', 'Python', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Education Details \\r\\n MCA   YMCAUST,  Faridab...</td>\n",
       "      <td>['Database']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>SKILLS C Basics, IOT, Python, MATLAB, Data Sci...</td>\n",
       "      <td>['Microsoft excel', 'Excel', 'C', 'Python', 'M...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Skills  Python  Tableau  Data Visualization  R...</td>\n",
       "      <td>['Excel', 'Statistics', 'Engineering', 'Ai', '...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Personal Skills  Ability to quickly grasp tech...</td>\n",
       "      <td>['Analysis', 'Numpy', 'Pandas', 'Engineering',...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Expertise , Data and Quantitative Analysis , D...</td>\n",
       "      <td>['Analyze', 'Ibm', 'System', 'Electronics', 'P...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Category                                             Resume  \\\n",
       "0   Data Science  Skills * Programming Languages: Python (pandas...   \n",
       "1   Data Science  Skills  R  Python  SAP HANA  Tableau  SAP HANA...   \n",
       "2   Data Science  Education Details \\r\\nMay 2013 to May 2017 B.E...   \n",
       "3   Data Science  Education Details \\r\\n B.Tech   Rayat and Bahr...   \n",
       "4   Data Science  Skills * Programming Languages: Python (pandas...   \n",
       "5   Data Science  Areas of Interest Deep Learning, Control Syste...   \n",
       "6   Data Science  Education Details \\r\\n MCA   YMCAUST,  Faridab...   \n",
       "7   Data Science  SKILLS C Basics, IOT, Python, MATLAB, Data Sci...   \n",
       "8   Data Science  Skills  Python  Tableau  Data Visualization  R...   \n",
       "9   Data Science  Personal Skills  Ability to quickly grasp tech...   \n",
       "10  Data Science  Expertise , Data and Quantitative Analysis , D...   \n",
       "\n",
       "                                                Skill  Exp  \n",
       "0   ['Interactive', 'Flask', 'Security', 'Ggplot',...  NaN  \n",
       "1   ['Telecom', 'Kpis', 'Servers', 'Sales', 'Engli...  NaN  \n",
       "2                         ['Research', 'Forecasting']  NaN  \n",
       "3   ['English', 'System', 'Python', 'Automation', ...  NaN  \n",
       "4   ['Interactive', 'Flask', 'Security', 'Ggplot',...  NaN  \n",
       "5   ['Flask', 'System', 'Electronics', 'Python', '...  NaN  \n",
       "6                                        ['Database']  NaN  \n",
       "7   ['Microsoft excel', 'Excel', 'C', 'Python', 'M...  NaN  \n",
       "8   ['Excel', 'Statistics', 'Engineering', 'Ai', '...  NaN  \n",
       "9   ['Analysis', 'Numpy', 'Pandas', 'Engineering',...  NaN  \n",
       "10  ['Analyze', 'Ibm', 'System', 'Electronics', 'P...  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Category'] == 'Data Science']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8103d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details \\r\\n\\r\\nData Science Assurance Associate \\r\\n\\r\\nData Science Assurance Associate - Ernst & Young LLP\\r\\nSkill Details \\r\\nJAVASCRIPT- Experience - 24 months\\r\\njQuery- Experience - 24 months\\r\\nPython- Experience - 24 monthsCompany Details \\r\\ncompany - Ernst & Young LLP\\r\\ndescription - Fraud Investigations and Dispute Services   Assurance\\r\\nTECHNOLOGY ASSISTED REVIE\\r\\nTAR (Technology Assisted Revie) assists in accelerating the revie process and run analytics and generate reports.\\r\\n* Core member of a team helped in developing automated revie platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating revies, resulting in reduced labor costs and time spent during the layers revie.\\r\\n* Understand the end to end flo of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. orked on analyzing the outputs and precision monitoring for the entire tool.\\r\\n* TAR assists in predictive coding, topic modelling from the evidence by folloing EY standards. Developed the classifier models in order to identify \"red flags\" and fraud-related issues.\\r\\n\\r\\nTools & Technologies: Python, scikit-learn, tfidf, ord2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.\\r\\n\\r\\nMULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)\\r\\nTEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIE DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.\\r\\n* Created heat map of terms by survey category based on frequency of ords * Extracted Positive and Negative ords across all the Survey categories and plotted ord cloud.\\r\\n* Created customized tableau dashboards for effective reporting and visualizations.\\r\\nCHATBOT * Developed a user friendly chatbot for one of our Products hich handle simple questions about hours of operation, reservation options and so on.\\r\\n* This chat bot serves entire product related questions. Giving overvie of tool via QA platform and also give recommendation responses so that user question to build chain of relevant anser.\\r\\n* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.\\r\\n\\r\\nTools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, ord Embedding, scikit-learn, JavaScript/JQuery, SqlServer\\r\\n\\r\\nINFORMATION GOVERNANCE\\r\\nOrganizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.\\r\\n* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.\\r\\n* Preforming ROT Analysis on the data hich give information of data hich helps identify content that is either Redundant, Outdated, or Trivial.\\r\\n* Preforming full-text search analysis on elastic search ith predefined methods hich can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) hich frequently targeted during cyber-attacks.\\r\\nTools & Technologies: Python, Flask, Elastic Search, Kibana\\r\\n\\r\\nFRAUD ANALYTIC PLATFORM\\r\\nFraud Analytics and investigative platform to revie all red flag cases.\\r\\n FAP is a Fraud Analytics and investigative platform ith inbuilt case manager and suite of Analytics for various ERP systems.\\r\\n* It can be used by clients to interrogate their Accounting systems for identifying the anomalies hich can be indicators of fraud by running advanced analytics\\r\\nTools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df['Resume'][0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2714d928",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\spanc/nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResume\u001b[39m\u001b[38;5;124m'\u001b[39m][index]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#     data = pd.rea\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(data)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<<Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresource_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m>>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[38;5;66;03m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[38;5;241m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[38;5;241m=\u001b[39m opened_resource\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[38;5;241m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m protocol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, path \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m protocol\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m find(path_, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mopen()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\spanc/nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\spanc\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    data = df['Resume'][index]\n",
    "#     data = pd.rea\n",
    "    tokens = nltk.word_tokenize(data)\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3209d0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
